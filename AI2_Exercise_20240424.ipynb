{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inagege/FKI/blob/main/AI2_Exercise_20240424.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise1 Text Modelling & Natural Language Understanding\n",
        "\n",
        "This exercise sheet will serve as supplementary to lectures and help you to familiarize yourself with Python, Tensroflow, and other commonly used packages.\n",
        "\n",
        "If you are not familiar with Python, you may want to learn more about Python\n",
        "and its basic syntax. Since there are a lot of free and well-written tutorials\n",
        " online, we refer you to one of the following online tutorials:\n",
        "\n",
        "* http://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook\n",
        "* https://www.learnpython.org/\n",
        "* https://www.w3schools.com/python/\n",
        "* https://automatetheboringstuff.com/\n"
      ],
      "metadata": {
        "id": "sDJf4y-6u4qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Firstly, let's download the data for this exercises\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
      ],
      "metadata": {
        "id": "yzweuCiE9478",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcaa7781-27c6-4487-948c-c90c9e4c8ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-24 09:11:04--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278779 (272K) [text/plain]\n",
            "Saving to: ‘botchan.txt’\n",
            "\n",
            "\rbotchan.txt           0%[                    ]       0  --.-KB/s               \rbotchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-04-24 09:11:04 (8.50 MB/s) - ‘botchan.txt’ saved [278779/278779]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words"
      ],
      "metadata": {
        "id": "UJCcM2ZzitcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bag of words is a representation of text that describes the occurrence of words within the corpus. Here we are going to have a hands-on exercise to better understand this approach.\n",
        "\n",
        "Given the following sentences:\n",
        "- OpenAI developed ChatGPT.\n",
        "- ChatGPT is a large language model.\n",
        "- Language technology is interestng.\n",
        "\n",
        "**Question**:\n",
        "\n",
        "\n",
        "1.   What is the vocabulary?\n",
        "2.   Encode these sentences with the vocabulary.\n",
        "\n"
      ],
      "metadata": {
        "id": "w9EZIi5Ii8qX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-gram Language Model\n",
        "\n",
        "Next, we are going to have a coding exercise about the n-gram language model. We have a corpus that is one chapter from a book and has almost 1000 lines. In this exercise, rather than building every function from scratch, we are going to use the toolkit NLTK, which is a suite of open-source Python modules, data sets, and tutorials supporting research and development in Natural Language Processing.\n",
        "\n",
        "For more information about NLTK, please refer to the official documentation: https://www.nltk.org/\n"
      ],
      "metadata": {
        "id": "fmCYuwngiydr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you are running with colab, this toolkit should be installed.\n",
        "import nltk\n",
        "print(nltk.__version__)\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "t8PT3d4E-OH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9da56bc-4acd-4ad0-e1e9-5bcd7004fbe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.8.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load punctuations that to be removed in later preprocessing.\n",
        "import nltk, re, pprint, string\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "puncs = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
        "print(puncs)\n",
        "\n",
        "puncs = puncs.replace('.', '')\n",
        "print(puncs)\n"
      ],
      "metadata": {
        "id": "DCfC-cLa40wP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33daf575-1d03-48c0-bc49-b3b2a6cbef60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~“”-’‘—\n",
            "!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~“”-’‘—\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the corpus\n",
        "file = open('botchan.txt', encoding = 'utf8').read()\n",
        "#preprocess data\n",
        "file_nl_removed = \"\"\n",
        "for line in file:\n",
        "  line_nl_removed = line.replace(\"\\n\", \" \")           #removes newlines\n",
        "  file_nl_removed += line_nl_removed\n",
        "\n",
        "file_p = \"\".join([char for char in file_nl_removed if char not in puncs])   #removes all special characters. Save the data as a string."
      ],
      "metadata": {
        "id": "y6lL2sNjFlUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_p[:1000] # Display the first 1000 elements"
      ],
      "metadata": {
        "id": "WmWKBl7n6Sql",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "952fe6e5-bdf0-4e62-e4ee-601990266dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffProject Gutenbergs Botchan Master Darling by Kinnosuke Natsume This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.  You may copy it give it away or reuse it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org Title Botchan Master Darling Author Kinnosuke Natsume Translator Yasotaro Morri Posting Date October 14 2012 EBook 8868 Release Date September 2005 First Posted August 17 2003 Language English  START OF THIS PROJECT GUTENBERG EBOOK BOTCHAN MASTER DARLING  Produced by David Starner and the Online Distributed Proofreading Team BOTCHAN MASTER DARLING By The Late Mr. Kinnosuke Natsume TRANSLATED By Yasotaro Morri Revised by J. R. KENNEDY 1919 A NOTE BY THE TRANSLATOR No translation can expect to equal much less to excel the original. The excellence of a translation can only be judged by noting how far it has succeeded in reproducing the original tone colors style the delicacy of senti'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check some statistics about the data. Here you need to complete the codes with reference: https://www.nltk.org/api/nltk.tokenize.html\n",
        "sents = nltk.sent_tokenize(file_p)\n",
        "print(\"The number of sentences is\", len(sents)) #prints the number of sentences\n",
        "\n",
        "words = nltk.word_tokenize(file_p)\n",
        "print(\"The number of tokens is\", len(words)) #prints the number of tokens\n",
        "\n",
        "average_tokens = round(len(words)/len(sents))\n",
        "print(\"The average number of tokens per sentence is\", average_tokens) #prints the average number of tokens per sentence\n",
        "\n",
        "unique_tokens = set(words)\n",
        "print(\"The number of unique tokens are\", len(unique_tokens)) #prints the number of unique tokens"
      ],
      "metadata": {
        "id": "VV3pSjmY6Lgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb91b81-195b-440b-8033-6ac8a461fe36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of sentences is 2787\n",
            "The number of tokens is 53568\n",
            "The average number of tokens per sentence is 19\n",
            "The number of unique tokens are 6293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's collect ngrams\n",
        "from nltk.util import ngrams #https://www.nltk.org/_modules/nltk/util.html#ngrams\n",
        "unigram=[]\n",
        "bigram=[]\n",
        "trigram=[]\n",
        "fourgram=[]\n",
        "tokenized_text = []\n",
        "\n",
        "for sentence in sents:\n",
        "    sentence = sentence.lower() # Lower the sentence for simplification\n",
        "    sequence = word_tokenize(sentence) # Word-level\n",
        "    for word in sequence:\n",
        "        if (word =='.'):\n",
        "            sequence.remove(word)\n",
        "        else:\n",
        "            unigram.append(word)\n",
        "    tokenized_text.append(sequence)\n",
        "    bigram.extend(list(ngrams(sequence, 2)))\n",
        "    trigram.extend(list(ngrams(sequence, 3)))\n",
        "    fourgram.extend(list(ngrams(sequence, 4)))"
      ],
      "metadata": {
        "id": "sfEL_tJTCfWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Lengths of unigram, bigram, trigram and fourgram are {len(unigram)}, {len(bigram)}, {len(trigram)}, {len(fourgram)}')"
      ],
      "metadata": {
        "id": "tZVHFOC2Ci2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcce2ad6-f86d-4a39-8d45-ef9952d813ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lengths of unigram, bigram, trigram and fourgram are 50796, 48009, 45246, 42515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect\n",
        "print(tokenized_text[:2])\n",
        "\n",
        "print(unigram[:5])\n",
        "print(bigram[:5])\n",
        "print(trigram[:5])\n",
        "print(fourgram[:5])"
      ],
      "metadata": {
        "id": "u6lAxDhKZcPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d94b722-50b6-41d5-afb5-4ed64ddb4d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['\\ufeffproject', 'gutenbergs', 'botchan', 'master', 'darling', 'by', 'kinnosuke', 'natsume', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever'], ['you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org', 'title', 'botchan', 'master', 'darling', 'author', 'kinnosuke', 'natsume', 'translator', 'yasotaro', 'morri', 'posting', 'date', 'october', '14', '2012', 'ebook', '8868', 'release', 'date', 'september', '2005', 'first', 'posted', 'august', '17', '2003', 'language', 'english', 'start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'botchan', 'master', 'darling', 'produced', 'by', 'david', 'starner', 'and', 'the', 'online', 'distributed', 'proofreading', 'team', 'botchan', 'master', 'darling', 'by', 'the', 'late', 'mr.', 'kinnosuke', 'natsume', 'translated', 'by', 'yasotaro', 'morri', 'revised', 'by', 'j.', 'r.', 'kennedy', '1919', 'a', 'note', 'by', 'the', 'translator', 'no', 'translation', 'can', 'expect', 'to', 'equal', 'much', 'less', 'to', 'excel', 'the', 'original']]\n",
            "['\\ufeffproject', 'gutenbergs', 'botchan', 'master', 'darling']\n",
            "[('\\ufeffproject', 'gutenbergs'), ('gutenbergs', 'botchan'), ('botchan', 'master'), ('master', 'darling'), ('darling', 'by')]\n",
            "[('\\ufeffproject', 'gutenbergs', 'botchan'), ('gutenbergs', 'botchan', 'master'), ('botchan', 'master', 'darling'), ('master', 'darling', 'by'), ('darling', 'by', 'kinnosuke')]\n",
            "[('\\ufeffproject', 'gutenbergs', 'botchan', 'master'), ('gutenbergs', 'botchan', 'master', 'darling'), ('botchan', 'master', 'darling', 'by'), ('master', 'darling', 'by', 'kinnosuke'), ('darling', 'by', 'kinnosuke', 'natsume')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_uni = nltk.FreqDist(unigram)\n",
        "freq_bi = nltk.FreqDist(bigram)\n",
        "freq_tri = nltk.FreqDist(trigram)\n",
        "freq_four = nltk.FreqDist(fourgram)\n",
        "\n",
        "print (\"Most common unigram: \", freq_uni.most_common(5))\n",
        "print (\"Most common bigrams: \", freq_bi.most_common(5))\n",
        "print (\"Most common trigrams: \", freq_tri.most_common(5))\n",
        "print (\"Most common fourgrams: \", freq_four.most_common(5))"
      ],
      "metadata": {
        "id": "aSbUY7cz-BhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d23852-72f0-4a0a-aad1-c2c28675b985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common unigram:  [('the', 2755), ('i', 1714), ('to', 1472), ('and', 1310), ('of', 1258)]\n",
            "Most common bigrams:  [(('of', 'the'), 316), (('in', 'the'), 223), (('to', 'the'), 189), (('red', 'shirt'), 174), (('i', 'was'), 137)]\n",
            "Most common trigrams:  [(('i', 'did', 'not'), 39), (('it', 'would', 'be'), 32), (('i', 'could', 'not'), 28), (('of', 'red', 'shirt'), 24), (('to', 'the', 'school'), 21)]\n",
            "Most common fourgrams:  [(('one', 'sen', 'and', 'a'), 13), (('sen', 'and', 'a', 'half'), 13), (('the', 'project', 'gutenberg', 'literary'), 13), (('project', 'gutenberg', 'literary', 'archive'), 13), (('gutenberg', 'literary', 'archive', 'foundation'), 13)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the next word\n",
        "str1 = 'i like the'\n",
        "token1 = word_tokenize(str1)\n",
        "\n",
        "num_gram = 2  # what if 4, 5\n",
        "\n",
        "results = list(ngrams(token1, num_gram))\n",
        "search=results[-1]\n",
        "print('Searching ', search)\n",
        "preds=[]\n",
        "\n",
        "# Naive search with a for loop\n",
        "\n",
        "for each in trigram:\n",
        "  if each[:-1]==search:\n",
        "    preds.append(each[-1])\n",
        "print('Found ', preds)\n",
        "\n",
        "# What to predict?\n",
        "from collections import Counter\n",
        "word_counts = Counter(preds)\n",
        "print('Most Common words with counts is: ', word_counts.most_common(1))"
      ],
      "metadata": {
        "id": "GuyWRWLqcvwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1308a147-b328-441f-97d0-151dad0a7975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching  ('like', 'the')\n",
            "Found  ['cheap', 'undue', 'dwarf', 'name', 'wrestling', 'inmates', 'tradespeople', 'wrestling']\n",
            "Most Common words with counts is:  [('wrestling', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Other popular ngram LM with scoring functions\n",
        "\n",
        "# https://github.com/kmario23/KenLM-training"
      ],
      "metadata": {
        "id": "73PSq7f0g_Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Byte Pair Encoding"
      ],
      "metadata": {
        "id": "K8HPYpBti4Fh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte Pair Encoding (BPE) was initially developed as an algorithm to compress data, and now is commonly used in natural language processing tasks such as machine translation and text generation. BPE starts by computing the unique set of characters used in the corpus, then building the vocabulary by taking all the symbols used to write those words.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r-LxM6kpGqA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calcuation\n",
        "\n",
        "Here we are going to implement BPE with a simple corpus step by step.\n",
        "\n",
        "Given the corpus like the following:\n",
        "\n",
        "- happy * 3\n",
        "- lucky * 2\n",
        "- cool * 2\n",
        "- good * 2\n",
        "\n",
        "You can inteprete each word as a single sentence. For example, the split of the words happy as {h_a_p_p_y_/w}\n",
        "\n",
        "**Questions**:\n",
        "\n",
        "\n",
        "1.   What is the initial vocabulary? Hint: don't forget the token **/w** at the end of each word.\n",
        "2.   What is the merge operation in the first iteration?\n",
        "3.   Let's say we would like to have a vocabulary with size 15. Calculate the vocabulary and the merging rules.\n"
      ],
      "metadata": {
        "id": "lEtih0VP9tws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Coding\n",
        "Next we are going to build a BPE model with [SentencePiece](https://github.com/google/sentencepiece/tree/master)."
      ],
      "metadata": {
        "id": "IGab9Cg19nzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coding exercise\n",
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "id": "eUMvy7QKjYYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3893c0-6a62-4537-df38-3e6de71684a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 3 botchan.txt # Display the first three rows"
      ],
      "metadata": {
        "id": "bkmTtaUqpoxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d99399c-7d71-4c01-fca8-44f6acb52cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿Project Gutenberg's Botchan (Master Darling), by Kin-nosuke Natsume\r\n",
            "This eBook is for the use of anyone anywhere at no cost and with\r\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
        "# `m.vocab` is just a reference. not used in the segmentation.\n",
        "spm.SentencePieceTrainer.train(input='botchan.txt', model_prefix='m', vocab_size=2000)\n",
        "\n",
        "# makes segmenter instance and loads the model file (m.model)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# encode: text => id\n",
        "print(sp.encode_as_pieces('This is a test'))\n",
        "print(sp.encode_as_ids('This is a test'))\n",
        "\n",
        "# decode: id => text\n",
        "print(sp.decode_pieces(sp.encode_as_pieces('This is a test')))\n",
        "print(sp.decode_ids(sp.encode_as_ids('This is a test')))"
      ],
      "metadata": {
        "id": "otyovEjXqGLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e5dcd3c-d34e-4486-bc2a-7139a78ae999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁This', '▁is', '▁a', '▁t', 'est']\n",
            "[208, 31, 9, 434, 601]\n",
            "This is a test\n",
            "This is a test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# returns vocab size\n",
        "print(sp.vocab_size())\n",
        "\n",
        "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
        "# <s> and </s> are defined as 'control' symbol.\n",
        "for id in range(3):\n",
        "  print(sp.id_to_piece(id))"
      ],
      "metadata": {
        "id": "ZXwomFHwqUjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bf558bc-8500-4a97-80bd-ae4a6a7fdd48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n",
            "<unk>\n",
            "<s>\n",
            "</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The impact of vocab size.\n",
        "for vocab_size in [100, 500, 5000]:\n",
        "  spm.SentencePieceTrainer.train(input='botchan.txt', model_prefix='m', vocab_size=vocab_size)\n",
        "  sp = spm.SentencePieceProcessor()\n",
        "  sp.load('m.model')\n",
        "  print(sp.encode_as_pieces('This is exercise 1'))\n",
        "\n",
        " # What if vocab_size is 50?"
      ],
      "metadata": {
        "id": "n-P08UlgqXMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dfbd49f-999c-466a-d96b-bc5ec5c5a060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', 'T', 'h', 'i', 's', '▁', 'i', 's', '▁', 'e', 'x', 'er', 'c', 'i', 's', 'e', '▁', '1']\n",
            "['▁This', '▁is', '▁ex', 'er', 'c', 'is', 'e', '▁', '1']\n",
            "['▁This', '▁is', '▁exercise', '▁1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26JvFrDZj_ti"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}